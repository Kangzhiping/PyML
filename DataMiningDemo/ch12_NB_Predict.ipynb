{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用朴素贝叶斯算法， MapReduce 框架\n",
    "# 预测博客博主的性别\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_search_re = re.compile(r\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ijij\n",
      "3\n",
      "2\n",
      "B\n",
      "haha\n",
      "C\n",
      "kang\n",
      "mike\n",
      "1234123j\n",
      "1\n",
      "ksjdflskd\n",
      "b\n",
      "A\n",
      "a\n",
      "123kj1k23j\n",
      "ewrkje\n",
      "c\n",
      "ksdjflksdjfk\n"
     ]
    }
   ],
   "source": [
    "words = word_search_re.findall(\"haha ijij 1234123j 123kj1k23j kang ksjdflskd ksdjflksdjfk ewrkje mike 1 2 3 a b c A B C 1 2\")\n",
    "print(type(words))\n",
    "for word in set(words):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename):\n",
    "    model = defaultdict(lambda: defaultdict(float))\n",
    "    with open(model_filename) as inf:\n",
    "        for line in inf:\n",
    "            word, values = line.split(maxsplit=1)\n",
    "            word = eval(word)\n",
    "            values = eval(values)\n",
    "            model[word] = values\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(os.path.expanduser(\"~\"), \"models\", \"part-00000\")\n",
    "model = load_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409.7987003114851, 513.3231594734408)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"i\"][\"male\"], model[\"i\"][\"female\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict(model, document):\n",
    "    words = word_search_re.findall(document)\n",
    "    probabilities = defaultdict(lambda : 0)\n",
    "    for word in set(words):\n",
    "        probabilities[\"male\"] += np.log(model[word].get(\"male\", 1e-5))\n",
    "        probabilities[\"female\"] += np.log(model[word].get(\"female\", 1e-5))\n",
    "    # Now find the most likely gender\n",
    "    most_likely_genders = sorted(probabilities.items(), key=itemgetter(1), reverse=True)\n",
    "    return most_likely_genders[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"\"\" Every day should be a half day.  Took the afternoon off to hit the dentist, and while I was out I managed to get my oil changed, too.  Remember that business with my car dealership this winter?  Well, consider this the epilogue.  The friendly fellas at the Valvoline Instant Oil Change on Snelling were nice enough to notice that my dipstick was broken, and the metal piece was too far down in its little dipstick tube to pull out.  Looks like I'm going to need a magnet.   Damn you, Kline Nissan, daaaaaaammmnnn yooouuuu....   Today I let my boss know that I've submitted my Corps application.  The news has been greeted by everyone in the company with a level of enthusiasm that really floors me.     The back deck has finally been cleared off by the construction company working on the place.  This company, for anyone who's interested, consists mainly of one guy who spends his days cursing at his crew of Spanish-speaking laborers.  Construction of my deck began around the time Nixon was getting out of office.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_predict(model, new_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"blogposts_testing\")\n",
    "testing_filenames = []\n",
    "for filename in os.listdir(testing_folder):\n",
    "    testing_filenames.append(os.path.join(testing_folder, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict_many(model, input_filename):\n",
    "    with open(input_filename) as inf:\n",
    "        # remove leading and trailing whitespace\n",
    "        for line in inf:\n",
    "            tokens = line.split()\n",
    "            actual_gender = eval(tokens[0])\n",
    "            blog_post = eval(\" \".join(tokens[1:]))\n",
    "            yield actual_gender, nb_predict(model, blog_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict(model, document):\n",
    "    words = word_search_re.findall(document)\n",
    "    probabilities = defaultdict(lambda : 1)\n",
    "    for word in set(words):\n",
    "        probabilities[\"male\"] += np.log(model[word].get(\"male\", 1e-15))\n",
    "        probabilities[\"female\"] += np.log(model[word].get(\"female\", 1e-15))\n",
    "    # Now find the most likely gender\n",
    "    most_likely_genders = sorted(probabilities.items(), key=itemgetter(1), reverse=True)\n",
    "    return most_likely_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for testing_filename in testing_filenames:\n",
    "    for actual_gender, ratios in nb_predict_many(model, testing_filename):\n",
    "        predicted_gender = ratios[0][0]\n",
    "        y_true.append(actual_gender == \"female\")\n",
    "        y_pred.append(predicted_gender == \"female\")\n",
    "y_true = np.array(y_true, dtype='int')\n",
    "y_pred = np.array(y_pred, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.5540\n",
      "acc=0.5765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1={:.4f}\".format(f1_score(y_true, y_pred, pos_label=None)))\n",
    "print(\"acc={:.4f}\".format(np.mean(y_true == y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_model_filename = os.path.join(os.path.expanduser(\"~\"), \"models\", \"model_aws\")\n",
    "aws_model = load_model(aws_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for testing_filename in testing_filenames:\n",
    "    for actual_gender, predicted_gender in nb_predict_many(aws_model, testing_filename):\n",
    "        predicted_gender = ratios[0][0]\n",
    "        y_true.append(actual_gender == \"female\")\n",
    "        y_pred.append(predicted_gender == \"female\")\n",
    "        #print(\"Actual: {0}\\tPredicted: {1}\".format(actual_gender, predicted_gender))\n",
    "        if len(y_true) > 500:\n",
    "            break\n",
    "y_true = np.array(y_true, dtype='int')\n",
    "y_pred = np.array(y_pred, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.8144\n",
      "acc=0.8734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"f1={:.4f}\".format(f1_score(y_true, y_pred, pos_label=None)))\n",
    "print(\"acc={:.4f}\".format(np.mean(y_true == y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(y_true, y_pred))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[614,   0],\n",
       "       [ 89,   0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}